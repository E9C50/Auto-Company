# Competitive Analysis: Deep Research Skill vs Market Leaders

## Competitive Landscape (2025)

### OpenAI Deep Research (o3-based)
- **Time**: 5-30 minutes
- **Sources**: Multi-step, unspecified count
- **Model**: o3 reasoning
- **Benchmark**: 26.6% on "Humanity's Last Exam"
- **Strengths**: Visual browser, transparency sidebar, reasoning capability
- **Weaknesses**: Slow, occasional hallucinations, may reference rumors

### Google Gemini Deep Research (2.5)
- **Time**: "A few minutes"
- **Sources**: "Hundreds of websites"
- **Model**: Gemini 2.5 Flash Thinking
- **Strengths**: PDF/image upload, Google Drive integration, interactive reports
- **Process**: Creates plan for approval before executing
- **Weaknesses**: Limited quality control

### Claude Desktop Research
- **Time**: "Less than a minute" (claimed)
- **Sources**: 427 sources in example (breadth over depth)
- **Strengths**: Speed, Google Workspace integration
- **Weaknesses**:
  - Often lacks cited sources for verification
  - Doesn't ask clarifying questions
  - Quality inconsistent
  - US/Japan/Brazil only, expensive ($100/mo Max plan)

---

## Our Deep Research Skill Advantages

### Speed Competitive
- **Standard Mode**: 5-10 minutes (faster than OpenAI, comparable to Gemini)
- **Quick Mode**: 2-5 minutes (approaches Claude Desktop speed)
- **Parallel Agents**: Simultaneous source retrieval for efficiency

### Superior Quality Control
| Feature | OpenAI | Gemini | Claude Desktop | **Our Skill** |
|---------|--------|--------|---------------|---------------|
| Source credibility scoring | ❌ | ❌ | ❌ | ✅ (0-100) |
| 3+ source triangulation | Partial | ❌ | ❌ | ✅ (enforced) |
| Built-in validation | ❌ | ❌ | ❌ | ✅ (automated) |
| Critique phase | ❌ | ❌ | ❌ | ✅ (red-team) |
| Refine phase | ❌ | ❌ | ❌ | ✅ (gap filling) |
| Citation quality | Good | Good | Poor | ✅ Excellent |

### Better Methodology
- **8-Phase Pipeline**: More thorough than competitors' ad-hoc approaches
- **Graph-of-Thoughts**: Non-linear reasoning with branching paths
- **Multiple Modes**: 4 depth levels (quick/standard/deep/ultradeep)
- **Decision Trees**: Clear logic for mode and tool selection
- **Stop Rules**: Prevents runaway research or low-quality loops

### Unique Differentiators

1. **Source Credibility Assessment**
   - Every source scored 0-100
   - Evaluates domain authority, recency, expertise, bias
   - Filters low-quality sources automatically

2. **Triangulation Phase**
   - Minimum 3 sources for major claims
   - Cross-reference verification
   - Flags contradictions explicitly

3. **Critique + Refine Cycle**
   - Red-team analysis before delivery
   - Identifies gaps and weaknesses
   - Iteratively improves before finalization

4. **Validation Infrastructure**
   - Automated quality checks
   - Catches placeholders, broken citations
   - Enforces quality standards

5. **Progressive Disclosure**
   - Tight SKILL.md (237 lines)
   - Detailed methodology in references
   - Efficient context management

### Performance Comparison

| Metric | OpenAI | Gemini | Claude Desktop | **Our Skill** |
|--------|--------|--------|----------------|---------------|
| **Speed** | 5-30 min | 2-5 min | <1 min | 2-10 min |
| **Source Count** | Unspecified | Hundreds | 427 | 15-50 |
| **Citation Quality** | Excellent | Good | Poor | Excellent |
